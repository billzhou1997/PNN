{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e349fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8146f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 2.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26259cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-31249df9d81b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "test=torch.tensor([[1,2],[3,4]],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=torch.tensor([[1.,2.],[3.,4.]])\n",
    "\n",
    "print(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7730e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a19e2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=torch.tensor([0.,0.],requires_grad=True)\n",
    "n=np.zeros(2)\n",
    "target=torch.tensor([10.,2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3f9f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(m):\n",
    "    y=m*a\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a42a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test1=net(n)\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8adf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52., grad_fn=<MseLossBackward>)\n",
      "tensor(51.7604, grad_fn=<MseLossBackward>)\n",
      "tensor(51.5216, grad_fn=<MseLossBackward>)\n",
      "tensor(51.2837, grad_fn=<MseLossBackward>)\n",
      "tensor(51.0466, grad_fn=<MseLossBackward>)\n",
      "tensor(50.8104, grad_fn=<MseLossBackward>)\n",
      "tensor(50.5750, grad_fn=<MseLossBackward>)\n",
      "tensor(50.3405, grad_fn=<MseLossBackward>)\n",
      "tensor(50.1069, grad_fn=<MseLossBackward>)\n",
      "tensor(49.8743, grad_fn=<MseLossBackward>)\n",
      "tensor(49.6425, grad_fn=<MseLossBackward>)\n",
      "tensor(49.4117, grad_fn=<MseLossBackward>)\n",
      "tensor(49.1819, grad_fn=<MseLossBackward>)\n",
      "tensor(48.9530, grad_fn=<MseLossBackward>)\n",
      "tensor(48.7250, grad_fn=<MseLossBackward>)\n",
      "tensor(48.4980, grad_fn=<MseLossBackward>)\n",
      "tensor(48.2720, grad_fn=<MseLossBackward>)\n",
      "tensor(48.0470, grad_fn=<MseLossBackward>)\n",
      "tensor(47.8229, grad_fn=<MseLossBackward>)\n",
      "tensor(47.5999, grad_fn=<MseLossBackward>)\n",
      "tensor(47.3779, grad_fn=<MseLossBackward>)\n",
      "tensor(47.1568, grad_fn=<MseLossBackward>)\n",
      "tensor(46.9368, grad_fn=<MseLossBackward>)\n",
      "tensor(46.7178, grad_fn=<MseLossBackward>)\n",
      "tensor(46.4998, grad_fn=<MseLossBackward>)\n",
      "tensor(46.2829, grad_fn=<MseLossBackward>)\n",
      "tensor(46.0669, grad_fn=<MseLossBackward>)\n",
      "tensor(45.8520, grad_fn=<MseLossBackward>)\n",
      "tensor(45.6381, grad_fn=<MseLossBackward>)\n",
      "tensor(45.4252, grad_fn=<MseLossBackward>)\n",
      "tensor(45.2134, grad_fn=<MseLossBackward>)\n",
      "tensor(45.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(44.7928, grad_fn=<MseLossBackward>)\n",
      "tensor(44.5840, grad_fn=<MseLossBackward>)\n",
      "tensor(44.3763, grad_fn=<MseLossBackward>)\n",
      "tensor(44.1695, grad_fn=<MseLossBackward>)\n",
      "tensor(43.9638, grad_fn=<MseLossBackward>)\n",
      "tensor(43.7591, grad_fn=<MseLossBackward>)\n",
      "tensor(43.5555, grad_fn=<MseLossBackward>)\n",
      "tensor(43.3528, grad_fn=<MseLossBackward>)\n",
      "tensor(43.1511, grad_fn=<MseLossBackward>)\n",
      "tensor(42.9505, grad_fn=<MseLossBackward>)\n",
      "tensor(42.7508, grad_fn=<MseLossBackward>)\n",
      "tensor(42.5521, grad_fn=<MseLossBackward>)\n",
      "tensor(42.3544, grad_fn=<MseLossBackward>)\n",
      "tensor(42.1577, grad_fn=<MseLossBackward>)\n",
      "tensor(41.9620, grad_fn=<MseLossBackward>)\n",
      "tensor(41.7672, grad_fn=<MseLossBackward>)\n",
      "tensor(41.5735, grad_fn=<MseLossBackward>)\n",
      "tensor(41.3806, grad_fn=<MseLossBackward>)\n",
      "tensor(41.1888, grad_fn=<MseLossBackward>)\n",
      "tensor(40.9979, grad_fn=<MseLossBackward>)\n",
      "tensor(40.8079, grad_fn=<MseLossBackward>)\n",
      "tensor(40.6189, grad_fn=<MseLossBackward>)\n",
      "tensor(40.4308, grad_fn=<MseLossBackward>)\n",
      "tensor(40.2437, grad_fn=<MseLossBackward>)\n",
      "tensor(40.0574, grad_fn=<MseLossBackward>)\n",
      "tensor(39.8721, grad_fn=<MseLossBackward>)\n",
      "tensor(39.6877, grad_fn=<MseLossBackward>)\n",
      "tensor(39.5042, grad_fn=<MseLossBackward>)\n",
      "tensor(39.3216, grad_fn=<MseLossBackward>)\n",
      "tensor(39.1398, grad_fn=<MseLossBackward>)\n",
      "tensor(38.9590, grad_fn=<MseLossBackward>)\n",
      "tensor(38.7790, grad_fn=<MseLossBackward>)\n",
      "tensor(38.6000, grad_fn=<MseLossBackward>)\n",
      "tensor(38.4217, grad_fn=<MseLossBackward>)\n",
      "tensor(38.2444, grad_fn=<MseLossBackward>)\n",
      "tensor(38.0678, grad_fn=<MseLossBackward>)\n",
      "tensor(37.8922, grad_fn=<MseLossBackward>)\n",
      "tensor(37.7173, grad_fn=<MseLossBackward>)\n",
      "tensor(37.5433, grad_fn=<MseLossBackward>)\n",
      "tensor(37.3701, grad_fn=<MseLossBackward>)\n",
      "tensor(37.1978, grad_fn=<MseLossBackward>)\n",
      "tensor(37.0262, grad_fn=<MseLossBackward>)\n",
      "tensor(36.8555, grad_fn=<MseLossBackward>)\n",
      "tensor(36.6856, grad_fn=<MseLossBackward>)\n",
      "tensor(36.5164, grad_fn=<MseLossBackward>)\n",
      "tensor(36.3480, grad_fn=<MseLossBackward>)\n",
      "tensor(36.1805, grad_fn=<MseLossBackward>)\n",
      "tensor(36.0137, grad_fn=<MseLossBackward>)\n",
      "tensor(35.8476, grad_fn=<MseLossBackward>)\n",
      "tensor(35.6823, grad_fn=<MseLossBackward>)\n",
      "tensor(35.5178, grad_fn=<MseLossBackward>)\n",
      "tensor(35.3540, grad_fn=<MseLossBackward>)\n",
      "tensor(35.1910, grad_fn=<MseLossBackward>)\n",
      "tensor(35.0287, grad_fn=<MseLossBackward>)\n",
      "tensor(34.8672, grad_fn=<MseLossBackward>)\n",
      "tensor(34.7063, grad_fn=<MseLossBackward>)\n",
      "tensor(34.5462, grad_fn=<MseLossBackward>)\n",
      "tensor(34.3868, grad_fn=<MseLossBackward>)\n",
      "tensor(34.2281, grad_fn=<MseLossBackward>)\n",
      "tensor(34.0702, grad_fn=<MseLossBackward>)\n",
      "tensor(33.9129, grad_fn=<MseLossBackward>)\n",
      "tensor(33.7563, grad_fn=<MseLossBackward>)\n",
      "tensor(33.6004, grad_fn=<MseLossBackward>)\n",
      "tensor(33.4451, grad_fn=<MseLossBackward>)\n",
      "tensor(33.2906, grad_fn=<MseLossBackward>)\n",
      "tensor(33.1367, grad_fn=<MseLossBackward>)\n",
      "tensor(32.9835, grad_fn=<MseLossBackward>)\n",
      "tensor(32.8310, grad_fn=<MseLossBackward>)\n",
      "tensor(32.6791, grad_fn=<MseLossBackward>)\n",
      "tensor(32.5278, grad_fn=<MseLossBackward>)\n",
      "tensor(32.3772, grad_fn=<MseLossBackward>)\n",
      "tensor(32.2273, grad_fn=<MseLossBackward>)\n",
      "tensor(32.0779, grad_fn=<MseLossBackward>)\n",
      "tensor(31.9292, grad_fn=<MseLossBackward>)\n",
      "tensor(31.7812, grad_fn=<MseLossBackward>)\n",
      "tensor(31.6337, grad_fn=<MseLossBackward>)\n",
      "tensor(31.4869, grad_fn=<MseLossBackward>)\n",
      "tensor(31.3407, grad_fn=<MseLossBackward>)\n",
      "tensor(31.1951, grad_fn=<MseLossBackward>)\n",
      "tensor(31.0501, grad_fn=<MseLossBackward>)\n",
      "tensor(30.9057, grad_fn=<MseLossBackward>)\n",
      "tensor(30.7619, grad_fn=<MseLossBackward>)\n",
      "tensor(30.6187, grad_fn=<MseLossBackward>)\n",
      "tensor(30.4761, grad_fn=<MseLossBackward>)\n",
      "tensor(30.3340, grad_fn=<MseLossBackward>)\n",
      "tensor(30.1926, grad_fn=<MseLossBackward>)\n",
      "tensor(30.0517, grad_fn=<MseLossBackward>)\n",
      "tensor(29.9114, grad_fn=<MseLossBackward>)\n",
      "tensor(29.7716, grad_fn=<MseLossBackward>)\n",
      "tensor(29.6325, grad_fn=<MseLossBackward>)\n",
      "tensor(29.4938, grad_fn=<MseLossBackward>)\n",
      "tensor(29.3558, grad_fn=<MseLossBackward>)\n",
      "tensor(29.2183, grad_fn=<MseLossBackward>)\n",
      "tensor(29.0813, grad_fn=<MseLossBackward>)\n",
      "tensor(28.9449, grad_fn=<MseLossBackward>)\n",
      "tensor(28.8091, grad_fn=<MseLossBackward>)\n",
      "tensor(28.6738, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5390, grad_fn=<MseLossBackward>)\n",
      "tensor(28.4047, grad_fn=<MseLossBackward>)\n",
      "tensor(28.2710, grad_fn=<MseLossBackward>)\n",
      "tensor(28.1378, grad_fn=<MseLossBackward>)\n",
      "tensor(28.0052, grad_fn=<MseLossBackward>)\n",
      "tensor(27.8731, grad_fn=<MseLossBackward>)\n",
      "tensor(27.7414, grad_fn=<MseLossBackward>)\n",
      "tensor(27.6103, grad_fn=<MseLossBackward>)\n",
      "tensor(27.4798, grad_fn=<MseLossBackward>)\n",
      "tensor(27.3497, grad_fn=<MseLossBackward>)\n",
      "tensor(27.2201, grad_fn=<MseLossBackward>)\n",
      "tensor(27.0911, grad_fn=<MseLossBackward>)\n",
      "tensor(26.9625, grad_fn=<MseLossBackward>)\n",
      "tensor(26.8345, grad_fn=<MseLossBackward>)\n",
      "tensor(26.7069, grad_fn=<MseLossBackward>)\n",
      "tensor(26.5799, grad_fn=<MseLossBackward>)\n",
      "tensor(26.4533, grad_fn=<MseLossBackward>)\n",
      "tensor(26.3272, grad_fn=<MseLossBackward>)\n",
      "tensor(26.2017, grad_fn=<MseLossBackward>)\n",
      "tensor(26.0766, grad_fn=<MseLossBackward>)\n",
      "tensor(25.9520, grad_fn=<MseLossBackward>)\n",
      "tensor(25.8278, grad_fn=<MseLossBackward>)\n",
      "tensor(25.7042, grad_fn=<MseLossBackward>)\n",
      "tensor(25.5810, grad_fn=<MseLossBackward>)\n",
      "tensor(25.4583, grad_fn=<MseLossBackward>)\n",
      "tensor(25.3361, grad_fn=<MseLossBackward>)\n",
      "tensor(25.2144, grad_fn=<MseLossBackward>)\n",
      "tensor(25.0931, grad_fn=<MseLossBackward>)\n",
      "tensor(24.9723, grad_fn=<MseLossBackward>)\n",
      "tensor(24.8520, grad_fn=<MseLossBackward>)\n",
      "tensor(24.7321, grad_fn=<MseLossBackward>)\n",
      "tensor(24.6127, grad_fn=<MseLossBackward>)\n",
      "tensor(24.4938, grad_fn=<MseLossBackward>)\n",
      "tensor(24.3753, grad_fn=<MseLossBackward>)\n",
      "tensor(24.2572, grad_fn=<MseLossBackward>)\n",
      "tensor(24.1397, grad_fn=<MseLossBackward>)\n",
      "tensor(24.0226, grad_fn=<MseLossBackward>)\n",
      "tensor(23.9059, grad_fn=<MseLossBackward>)\n",
      "tensor(23.7897, grad_fn=<MseLossBackward>)\n",
      "tensor(23.6739, grad_fn=<MseLossBackward>)\n",
      "tensor(23.5586, grad_fn=<MseLossBackward>)\n",
      "tensor(23.4437, grad_fn=<MseLossBackward>)\n",
      "tensor(23.3293, grad_fn=<MseLossBackward>)\n",
      "tensor(23.2153, grad_fn=<MseLossBackward>)\n",
      "tensor(23.1018, grad_fn=<MseLossBackward>)\n",
      "tensor(22.9887, grad_fn=<MseLossBackward>)\n",
      "tensor(22.8760, grad_fn=<MseLossBackward>)\n",
      "tensor(22.7638, grad_fn=<MseLossBackward>)\n",
      "tensor(22.6520, grad_fn=<MseLossBackward>)\n",
      "tensor(22.5406, grad_fn=<MseLossBackward>)\n",
      "tensor(22.4297, grad_fn=<MseLossBackward>)\n",
      "tensor(22.3192, grad_fn=<MseLossBackward>)\n",
      "tensor(22.2092, grad_fn=<MseLossBackward>)\n",
      "tensor(22.0995, grad_fn=<MseLossBackward>)\n",
      "tensor(21.9903, grad_fn=<MseLossBackward>)\n",
      "tensor(21.8816, grad_fn=<MseLossBackward>)\n",
      "tensor(21.7732, grad_fn=<MseLossBackward>)\n",
      "tensor(21.6653, grad_fn=<MseLossBackward>)\n",
      "tensor(21.5578, grad_fn=<MseLossBackward>)\n",
      "tensor(21.4507, grad_fn=<MseLossBackward>)\n",
      "tensor(21.3440, grad_fn=<MseLossBackward>)\n",
      "tensor(21.2377, grad_fn=<MseLossBackward>)\n",
      "tensor(21.1319, grad_fn=<MseLossBackward>)\n",
      "tensor(21.0265, grad_fn=<MseLossBackward>)\n",
      "tensor(20.9215, grad_fn=<MseLossBackward>)\n",
      "tensor(20.8169, grad_fn=<MseLossBackward>)\n",
      "tensor(20.7127, grad_fn=<MseLossBackward>)\n",
      "tensor(20.6090, grad_fn=<MseLossBackward>)\n",
      "tensor(20.5056, grad_fn=<MseLossBackward>)\n",
      "tensor(20.4026, grad_fn=<MseLossBackward>)\n",
      "tensor(20.3001, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.Adam([m], lr=0.01, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.001, amsgrad=False)  #adam optimizer\n",
    "criterion=nn.MSELoss()\n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(m)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8174c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.8303, 0.9997], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d7c3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size=10\n",
    "ez=np.zeros((size,size))\n",
    "hx=np.zeros((size,size))\n",
    "hy=np.zeros((size,size))\n",
    "Eps=np.ones((size,size))\n",
    "Eps=torch.tensor(Eps)\n",
    "ez=torch.tensor(ez)\n",
    "hx=torch.tensor(hx)\n",
    "hy=torch.tensor(hy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28f2b0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(Eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "156f214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fieldupdate3(T=10,size=100,E=ez,Hx=hx,Hy=hy,Eps=Eps):\n",
    "    imp0=377\n",
    "#     E_history=[]\n",
    "#     Hx_history=[]\n",
    "#     Hy_history=[]\n",
    "#     test=[]\n",
    "    for t in range(T):\n",
    "        for i in range(size):\n",
    "            for j in range(size-1):\n",
    "                Hx[i][j]=Hx[i][j]+(E[i][j+1]-E[i][j])/imp0/2/Eps[i][j]\n",
    "\n",
    "        for i in range(size-1):\n",
    "            for j in range(size):\n",
    "                \n",
    "                Hy[i][j]=Hy[i][j]+(E[i+1][j]-E[i][j])/imp0/2\n",
    "                \n",
    "                \n",
    "        for i in range(size-1):\n",
    "            for j in range(size-1):\n",
    "                E[i+1][j+1]=E[i+1][j+1]+(Hx[i+1][j+1]+Hy[i+1][j+1]-Hx[i+1][j]-Hy[i][j+1])*imp0/2\n",
    "        #E[int(size/4)][int(size/4)]+=math.sin(0.1*t)\n",
    "        E[int(size/2)][int(size/2)]+=math.sin(0.1*t)\n",
    "        E[int(size/2)][int(size/2)+1]+=math.sin(0.1*t)\n",
    "        #E[int(size/2)][int(size/2)]+=math.exp(-(t+1-0.3*size)*(t+1-0.3*size)/100)\n",
    "#         test.append(E[int(0.5*size)])\n",
    "#         E_history.append(E.copy())\n",
    "#         Hx_history.append(Hx.copy())\n",
    "#         Hy_history.append(Hy.copy())\n",
    "#     with open(\"E field history.txt\", 'w') as f:\n",
    "#         for member in E_history:\n",
    "#             f.write(str(member) + '\\n')\n",
    "#     with open(\"Hx field history.txt\", 'w') as f:\n",
    "#         for member in Hx_history:\n",
    "#             f.write(str(member) + '\\n')\n",
    "#     with open(\"Hy field history.txt\", 'w') as f:\n",
    "#         for member in Hy_history:\n",
    "#             f.write(str(member) + '\\n')\n",
    "#     E_history=np.array(E_history)\n",
    "        \n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42ae325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.7961711883544922 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "E=fieldupdate3(100,size,ez,hx,hy)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9790b25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27aef829760>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.imshow(E,origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ce482691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-9., -8., -7., -6., -5., -4., -3., -2., -1.,  0.], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "M=np.ones((size,size))\n",
    "M=torch.tensor(M,requires_grad=True)\n",
    "target=torch.tensor([1.,2.,3.,4.,5.,6.,7.,8.,9.,10.],dtype=torch.float64)\n",
    "#target=target.unsqueeze(1)\n",
    "X=torch.ones([10,1], dtype=torch.float64)\n",
    "print(target.size())\n",
    "pred=torch.matmul(M,X)\n",
    "pred=pred.reshape([10,])\n",
    "print(pred.size())\n",
    "print(target-pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bdeb8979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(28.5000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.Adam([X], lr=0.01, betas=(0.9, 0.999), eps=1e-9, weight_decay=0.1, amsgrad=False)  #adam optimizer\n",
    "criterion=nn.MSELoss()\n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = torch.matmul(M,X)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "94d5538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8f30965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([4., 7.], dtype=torch.float64, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "M=np.ones((2,2))\n",
    "M=torch.tensor(M,requires_grad=True)\n",
    "target=torch.tensor([7.,10.],dtype=torch.float64)\n",
    "#target=target.unsqueeze(1)\n",
    "X=torch.tensor([1.,2.], dtype=torch.float64)\n",
    "print(target.size())\n",
    "pred=torch.matmul(M,X)\n",
    "pred=pred.reshape([2,])\n",
    "print(target-pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02d75fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5846, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5083, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.4339, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3613, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2907, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2220, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1551, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0902, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0271, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9658, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9063, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8484, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7920, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7370, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6834, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6310, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5797, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5294, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4800, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4316, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3841, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3375, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2918, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2470, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2032, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1605, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1187, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0781, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0385, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9625, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9262, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8910, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8567, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8236, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7914, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7602, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7299, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7005, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6720, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6444, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6176, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5917, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5666, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5422, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5187, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4960, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4741, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4529, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4325, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4129, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3939, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3757, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3582, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3413, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3251, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3095, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2945, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2801, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2663, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2530, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2403, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2281, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2164, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2052, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1945, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1843, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1745, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1651, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1562, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1477, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1396, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1245, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1174, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1044, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0983, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0926, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0871, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0770, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0723, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0679, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0637, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0597, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0560, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0525, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0491, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0459, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0430, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0401, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0375, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0350, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0326, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0304, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0284, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0264, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0228, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0212, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0197, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0183, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0170, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0157, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0125, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0107, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0098, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0091, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(9.7305e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(8.6675e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(7.7178e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(6.8699e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1136e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4395e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8391e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(4.3048e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.8297e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.4076e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.0329e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.7006e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.4063e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1458e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9156e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7124e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5331e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3752e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2363e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1142e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0070e-05, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(9.1301e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(8.3058e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(7.5835e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(6.9509e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(6.3968e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9113e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4857e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1123e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7844e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4962e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2425e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(4.0192e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.8224e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.6491e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.4966e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.3626e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.2452e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.1428e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(3.0539e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.9774e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.9121e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(2.8570e-06, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.Adam({M}, lr=0.01, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.001, amsgrad=False)  #adam optimizer\n",
    "criterion=nn.MSELoss()\n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = torch.matmul(M,X)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0d0540ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3118, 2.3433],\n",
      "        [3.3313, 3.3335]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ac3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
