{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a7aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c92988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlist\n",
      "exp_out_list\n"
     ]
    }
   ],
   "source": [
    "data = np.load('exp_io_data\\CoupledPendula_mean_in7_out7_Tmax2.0_data.npz')\n",
    "for key, val in data.items():\n",
    "    print(key)\n",
    "    exec(key +'=val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f5386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=xlist\n",
    "y=exp_out_list\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6c4943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 14)\n",
      "(2, 60, 2000, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6fc4b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 60, 2000, 7, 2)\n",
      "torch.Size([1, 1000, 60, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "#y1=y[0:1,29:30,:,:,0:1]\n",
    "print(y.shape)\n",
    "\n",
    "#(Nrepeat, Ntimesteps, Nx, output_dim, 2)  <-- 2 = (final position, final velocity)\n",
    "#y1_np=torch.from_numpy(y1)\n",
    "#y1=torch.transpose(y1_np,1,2)\n",
    "#print(y1.shape)\n",
    "y2=y[0:1,:,0:1000,:,0:1]\n",
    "#(Nrepeat, Ntimesteps, Nx, output_dim, 2)  <-- 2 = (final position, final velocity)\n",
    "y2_np=torch.from_numpy(y2)\n",
    "y2=torch.transpose(y2_np,1,2)\n",
    "print(y2.shape)\n",
    "#print(y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81970138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47792e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 60, 7])\n"
     ]
    }
   ],
   "source": [
    "y3=y2.squeeze(0).squeeze(3)\n",
    "print(y3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7683a1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "y_train=y3[:,3:4,:]\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f7cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 14)\n",
      "[[0.9515082  0.02859658 0.36347437 ... 0.35507327 0.8292224  0.4047569 ]\n",
      " [0.25135255 0.7703726  0.775273   ... 0.14982301 0.15127295 0.7362834 ]\n",
      " [0.19514024 0.16856277 0.07611024 ... 0.7874241  0.22519547 0.09913057]\n",
      " ...\n",
      " [0.47233897 0.5813042  0.5970881  ... 0.4770273  0.8321323  0.0551641 ]\n",
      " [0.9792367  0.9304237  0.7407218  ... 0.07495439 0.42210817 0.49420857]\n",
      " [0.77542037 0.6565631  0.83288056 ... 0.03630865 0.04533863 0.08364367]]\n"
     ]
    }
   ],
   "source": [
    "x_train=x[0:1000,:]\n",
    "print(x_train.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c153c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Linear(in_features=14, out_features=14, bias=True)\n",
      "  (layer2): Linear(in_features=14, out_features=14, bias=True)\n",
      "  (layer3): Linear(in_features=14, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(14, 14 ,bias=True)\n",
    "        self.layer2 = nn.Linear(14, 14,bias=True)\n",
    "        self.layer3 = nn.Linear(14, 7,bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = x+0.1*self.layer1(x)\n",
    "        x = x+0.1*self.layer2(x)\n",
    "        x = torch.sigmoid(self.layer3(x))\n",
    "        #x = F.relu(self.layer4(x))\n",
    "        #x = F.relu(self.layer5(x))\n",
    "        #x = torch.sigmoid(self.layer6(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f687626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([14, 14])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9490aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4259, 0.6515, 0.5182,  ..., 0.5584, 0.5824, 0.4887],\n",
      "        [0.5059, 0.6381, 0.5155,  ..., 0.5571, 0.5858, 0.5471],\n",
      "        [0.5772, 0.6640, 0.4657,  ..., 0.5410, 0.4751, 0.4539],\n",
      "        ...,\n",
      "        [0.5624, 0.6671, 0.5327,  ..., 0.6123, 0.5856, 0.5003],\n",
      "        [0.5302, 0.6058, 0.5137,  ..., 0.5618, 0.5126, 0.5298],\n",
      "        [0.6179, 0.6268, 0.5602,  ..., 0.5695, 0.4377, 0.4946]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "torch.Size([1000, 7])\n"
     ]
    }
   ],
   "source": [
    "x_data=torch.tensor(x_train)\n",
    "input = x_data\n",
    "out = net(input)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72a86957",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1000,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdb550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df172deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 7])\n",
      "tensor(0.8105, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target=y_train\n",
    "target_new=target.view(1000,7)\n",
    "print(target_new.shape)\n",
    "criterion=nn.MSELoss()\n",
    "loss=criterion(out,target_new)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a6e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5a53db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5851, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5862, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5872, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5876, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5874, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5870, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5860, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5858, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5858, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5860, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5863, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5867, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5869, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5871, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5870, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5869, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5862, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5861, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5862, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5863, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5867, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5868, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5868, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5868, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5867, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5863, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5863, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5867, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5867, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5867, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5865, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.Adam(params, lr=0.01, betas=(0.9, 0.999), eps=1e-09, weight_decay=0.1, amsgrad=False)  #adam optimizer\n",
    "    \n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input)\n",
    "    loss = criterion(output, target_new)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cba61ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 14)\n"
     ]
    }
   ],
   "source": [
    "x_test=x[1010:1020,:]\n",
    "print(x_test.shape)\n",
    "y1=y[0:1,:,1010:1020,:,0:1]\n",
    "#(Nrepeat, Ntimesteps, Nx, output_dim, 2)  <-- 2 = (final position, final velocity)\n",
    "y1_np=torch.from_numpy(y1)\n",
    "y1=torch.transpose(y1_np,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae78d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 60, 7])\n",
      "torch.Size([10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "y4=y1.squeeze(0).squeeze(3)\n",
    "print(y4.shape)\n",
    "\n",
    "y_test=y4[:,29:30,3:4]\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "122b0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3317, 0.3297, 0.3363, 0.3311, 0.3287, 0.3300, 0.3321],\n",
      "        [0.3459, 0.3321, 0.3501, 0.3301, 0.3271, 0.3614, 0.3401],\n",
      "        [0.2821, 0.2818, 0.2937, 0.2800, 0.2697, 0.2847, 0.2920],\n",
      "        [0.3477, 0.3325, 0.3445, 0.3321, 0.3245, 0.3381, 0.3588],\n",
      "        [0.3653, 0.3586, 0.3638, 0.3558, 0.3573, 0.3843, 0.3662],\n",
      "        [0.2942, 0.2795, 0.3090, 0.2992, 0.2730, 0.3037, 0.3077],\n",
      "        [0.2964, 0.2910, 0.3216, 0.3077, 0.3009, 0.3221, 0.3171],\n",
      "        [0.2981, 0.3097, 0.3130, 0.2947, 0.2960, 0.3070, 0.3144],\n",
      "        [0.3355, 0.3374, 0.3487, 0.3207, 0.3199, 0.3288, 0.3325],\n",
      "        [0.2995, 0.2949, 0.2904, 0.2861, 0.2849, 0.2890, 0.2957]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[-0.0254]],\n",
      "\n",
      "        [[-0.5383]],\n",
      "\n",
      "        [[-0.1750]],\n",
      "\n",
      "        [[-1.9699]],\n",
      "\n",
      "        [[-0.8048]],\n",
      "\n",
      "        [[-1.3674]],\n",
      "\n",
      "        [[-0.3631]],\n",
      "\n",
      "        [[-0.1402]],\n",
      "\n",
      "        [[ 0.1298]],\n",
      "\n",
      "        [[ 0.5736]]])\n"
     ]
    }
   ],
   "source": [
    "x_test=torch.tensor(x_test)\n",
    "test_out=net(x_test)\n",
    "#LOSS = criterion(test_out, y_test)\n",
    "print(test_out)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb087b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(14, 120)\n",
    "        self.layer2 = nn.Linear(120, 120)\n",
    "        self.layer3 = nn.Linear(120, 120)\n",
    "        self.layer4 = nn.Linear(120, 84)\n",
    "        self.layer5 = nn.Linear(84, 42)\n",
    "        self.layer6 = nn.Linear(42, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = x+F.relu(self.layer2(x))\n",
    "        x = x+F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.tanh(self.layer5(x))\n",
    "        x = F.relu(self.layer6(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net2 = Net2()\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c01430",
   "metadata": {},
   "source": [
    "params2 = list(net2.parameters())\n",
    "print(len(params2))\n",
    "print(params2[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b558da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=torch.tensor(x)\n",
    "input = x_data\n",
    "out = net2(input)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ad80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=y1[0:1000,:,:]\n",
    "target_new=target.view(1000,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-06, weight_decay=0.1, amsgrad=False)  #adam optimizer\n",
    "    \n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net2(input)\n",
    "    loss = criterion(output, target_new)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684b295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
