{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a7aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c92988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlist\n",
      "exp_out_list\n"
     ]
    }
   ],
   "source": [
    "data = np.load('exp_io_data\\CoupledPendula_mean_in7_out7_Tmax1.0_data.npz')\n",
    "for key, val in data.items():\n",
    "    print(key)\n",
    "    exec(key +'=val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f5386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 14)\n",
      "(2, 30, 2000, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "x=xlist\n",
    "y=exp_out_list\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbad5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 7)\n"
     ]
    }
   ],
   "source": [
    "x_in=x[:,0:7]\n",
    "x_para=x[:,7:14]\n",
    "#print(x_in)\n",
    "#print(x_para)\n",
    "v_int=np.zeros((2000,7))\n",
    "x_train=x_in[0:1000,:]\n",
    "x_train2=x[0:1000,:]\n",
    "print(x_train.shape)\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe5e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 2000, 7)\n",
      "(29, 2000, 7)\n",
      "(30, 2000, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_temp2=y[0,0:29,:,:,0]\n",
    "y_temp1=y[0,1:30,:,:,0]\n",
    "print(y_temp2.shape)\n",
    "print(y_temp1.shape)\n",
    "y_v=(y_temp2-y_temp1)#/(1/30)\n",
    "y_0=np.zeros((1,2000,7))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_v=np.concatenate((y_0,y_v),axis=0)\n",
    "print(y_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6fc4b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30, 2000, 7, 2)\n",
      "torch.Size([2000, 30, 14])\n"
     ]
    }
   ],
   "source": [
    "#y1=y[0:1,29:30,:,:,0:1]\n",
    "print(y.shape)\n",
    "y_x=y[0,:,:,:,0]\n",
    "y_new=np.concatenate((y_x,y_v),axis=2)\n",
    "y_new=torch.from_numpy(y_new)\n",
    "y_new=torch.transpose(y_new,0,1)\n",
    "print(y_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca18076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.9990e-01,  3.9747e-01,  6.3987e-01, -1.6053e-01, -2.1341e-01,\n",
      "         -8.2006e-01,  5.4416e-01,  4.6262e-02,  6.3472e-03,  2.7629e-02,\n",
      "         -1.1352e-02,  4.7393e-04, -5.0331e-02,  3.8788e-02]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[ 7.6473e-01,  3.7754e-01,  5.5941e-01, -1.2792e-01, -2.1344e-01,\n",
      "         -6.7457e-01,  4.3281e-01,  1.3517e-01,  1.9928e-02,  8.0459e-02,\n",
      "         -3.2603e-02,  2.7239e-05, -1.4549e-01,  1.1134e-01]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(y_new[1,1:2,:])\n",
    "print(y_new[1,2:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a478b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 14])\n"
     ]
    }
   ],
   "source": [
    "F=3 #Fth frame\n",
    "\n",
    "y_train=y_new[0:1000,F:F+1,:]\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c153c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Linear(in_features=14, out_features=14, bias=True)\n",
      "  (layer2): Linear(in_features=14, out_features=14, bias=True)\n",
      "  (layer3): Linear(in_features=14, out_features=14, bias=True)\n",
      "  (layer4): Linear(in_features=14, out_features=14, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(14, 14 ,bias=True)\n",
    "        self.layer2 = nn.Linear(14, 14,bias=True)\n",
    "        self.layer3 = nn.Linear(14, 14,bias=True)\n",
    "        #self.layer4 = nn.Linear(14, 14,bias=True)\n",
    "        #self.layer5 = nn.Linear(14, 14,bias=True)\n",
    "        self.layer4 = nn.Linear(14, 14,bias=True)\n",
    "    def forward(self, x,activation=\"RELU\"):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = x+torch.sigmoid(self.layer1(x))\n",
    "        x = x+torch.sigmoid(self.layer2(x))\n",
    "        x = x+torch.sigmoid(self.layer3(x))\n",
    "        #x = x+torch.sigmoid(self.layer4(x))\n",
    "        #x = x+torch.sigmoid(self.layer5(x))\n",
    "        x = self.layer4(x)\n",
    "        #x = F.relu(self.layer4(x))\n",
    "        #x = F.relu(self.layer5(x))\n",
    "        #x = torch.sigmoid(self.layer6(x))\n",
    "        return x.float()\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(14, 14 ,bias=True)\n",
    "        self.layer2 = nn.Linear(14, 14,bias=True)\n",
    "        self.layer3 = nn.Linear(14, 14,bias=True)\n",
    "        #self.layer4 = nn.Linear(14, 14,bias=True)\n",
    "        #self.layer5 = nn.Linear(14, 14,bias=True)\n",
    "        self.layer4 = nn.Linear(14, 7,bias=True)\n",
    "    def forward(self, x,activation=\"RELU\"):\n",
    "        \n",
    "        if activation==\"RELU\":\n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "            x = x+0.1*(self.layer1(x))\n",
    "            x = x+0.1*(self.layer2(x))\n",
    "            x = x+0.1*(self.layer3(x))\n",
    "            #x = x+torch.sigmoid(self.layer4(x))\n",
    "            #x = x+torch.sigmoid(self.layer5(x))\n",
    "            x = self.layer4(x)\n",
    "            #x = F.relu(self.layer4(x))\n",
    "            #x = F.relu(self.layer5(x))\n",
    "            #x = torch.sigmoid(self.layer6(x))\n",
    "        if activation==\"SIG\":\n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "            x = x+torch.sigmoid(self.layer1(x))\n",
    "            x = x+torch.sigmoid(self.layer2(x))\n",
    "            x = x+torch.sigmoid(self.layer3(x))\n",
    "            #x = x+torch.sigmoid(self.layer4(x))\n",
    "            #x = x+torch.sigmoid(self.layer5(x))\n",
    "            x = self.layer4(x)\n",
    "            #x = F.relu(self.layer4(x))\n",
    "            #x = F.relu(self.layer5(x))\n",
    "            #x = torch.sigmoid(self.layer6(x))            \n",
    "        return x.float()\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f687626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([14, 14])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9490aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9795, -1.4794, -1.1413,  ..., -1.4645, -0.9520, -2.6713],\n",
      "        [-2.8495, -1.8427, -1.0279,  ..., -1.5492, -0.7971, -2.3189],\n",
      "        [-2.8449, -1.7865, -0.9964,  ..., -1.3630, -0.4737, -2.4789],\n",
      "        ...,\n",
      "        [-2.8814, -1.5936, -0.7804,  ..., -1.1017, -0.6299, -2.2588],\n",
      "        [-2.8584, -2.0040, -0.7086,  ..., -1.1401, -0.9443, -2.4606],\n",
      "        [-2.7919, -1.9122, -0.9796,  ..., -1.3159, -0.6416, -2.4778]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "torch.Size([1000, 14])\n"
     ]
    }
   ],
   "source": [
    "x_data=torch.tensor(x_train2)\n",
    "input = x_data\n",
    "out = net(input,\"SIG\")\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72a86957",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1000,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdb550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df172deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 14])\n",
      "tensor(2.3995, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target=y_train\n",
    "target_new=target.view(1000,14).float()\n",
    "print(target_new.shape)\n",
    "criterion=nn.MSELoss()\n",
    "loss=criterion(out,target_new)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "685a6e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-81f18ba176eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msum_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msum_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_layers' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(60/num_layers):\n",
    "    loss\n",
    "    sum_loss+=loss\n",
    "    sum_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5a53db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2184, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2315, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2324, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2256, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2256, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2260, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2206, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2191, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2227, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2229, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2189, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2172, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2193, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2205, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2191, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2177, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2179, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2185, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2183, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2180, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2178, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2176, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2174, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2175, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2176, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2175, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2172, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2171, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2173, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2173, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2171, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2171, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2171, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2169, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2169, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2169, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2169, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2169, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2169, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2169, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.Adam(params, lr=0.01, betas=(0.9, 0.999), eps=1e-09, weight_decay=0.1, amsgrad=False)  #adam optimizer\n",
    "    \n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input)\n",
    "    loss = criterion(output, target_new).float()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68929243",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_new.shape)\n",
    "y_test=y_new[1000:2000,F:F+1,:]\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c198fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=torch.tensor(x_test)\n",
    "test_out=net(x_test)\n",
    "print(test_out.shape)\n",
    "#LOSS = criterion(test_out, y_test)\n",
    "# print(test_out)\n",
    "# print(y_test)\n",
    "y_test_new=y_test.squeeze(1)\n",
    "print(y_test_new.shape)\n",
    "# test_loss=criterion(y_test_new,test_out)\n",
    "# print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test_new.flatten(), 5*test_out.detach().numpy().flatten(), '.')\n",
    "plt.title('Training')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7489bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dt_exp_comparison(yexp, ypred):\n",
    "\n",
    "    fig, axs = plt.subplots(nrows = 3, ncols = 3, figsize = [10,8], dpi = 100, sharex=True, sharey=True)\n",
    "    for i in range(9):\n",
    "        plt.sca(axs.flatten()[i])\n",
    "        plt.plot(ypred[i], '.-', lw = 1, c = 'k', alpha = 0.5, label = 'digital twin prediction')\n",
    "        plt.plot(yexp[i], '.-', lw = 1, c = 'b', alpha = 0.5, label = 'experimental outcome')\n",
    "        plt.xlabel('# pendulum')\n",
    "        plt.ylabel('output angle')\n",
    "        plt.title(f'Initial conditions {i}')\n",
    "        plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('img/coupled_pendula_dt_examples.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_dt_exp_comparison(y_test_new,10*test_out.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb087b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(14, 120)\n",
    "        self.layer2 = nn.Linear(120, 120)\n",
    "        self.layer3 = nn.Linear(120, 120)\n",
    "        self.layer4 = nn.Linear(120, 84)\n",
    "        self.layer5 = nn.Linear(84, 42)\n",
    "        self.layer6 = nn.Linear(42, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = x+F.relu(self.layer2(x))\n",
    "        x = x+F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.tanh(self.layer5(x))\n",
    "        x = F.relu(self.layer6(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net2 = Net2()\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c01430",
   "metadata": {},
   "source": [
    "params2 = list(net2.parameters())\n",
    "print(len(params2))\n",
    "print(params2[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b558da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=torch.tensor(x)\n",
    "input = x_data\n",
    "out = net2(input)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ad80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=y1[0:1000,:,:]\n",
    "target_new=target.view(1000,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-06, weight_decay=0.1, amsgrad=False)  #adam optimizer\n",
    "    \n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net2(input)\n",
    "    loss = criterion(output, target_new)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684b295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
